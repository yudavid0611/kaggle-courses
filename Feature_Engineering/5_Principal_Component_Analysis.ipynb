{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명\n",
    "- 데이터의 variation을 분할한다.\n",
    "    - 표준화된 데이터에서 variation은 `correlation`을 의미한다.\n",
    "    - 표준화되지 않은 데이터에서 variation은 `covariance`를 의미한다.\n",
    "- 일반적으로 표준화된 데이터에 적용한다.\n",
    "- PCA는 기존 features로 데이터를 설명하는 것을 대신하여 `variation의 축(axes)`으로 설명한다.\n",
    "\n",
    "    <img src=\"assets/pca1.png\" width=\"50%\"/>\n",
    "\n",
    "    <img src=\"assets/pca2.png\" width=\"70%\"/>\n",
    "\n",
    "    - PCA로 만든 새로운 features는 기존 features의 선형 결합이다.\n",
    "    ``` python\n",
    "    df[\"Size\"] = 0.707 * X[\"Height\"] + 0.707 * X[\"Diameter\"]\n",
    "    df[\"Shape\"] = 0.707 * X[\"Height\"] - 0.707 * X[\"Diameter\"]\n",
    "    ```\n",
    "    - 새로운 features는 데이터의 `principal components`라고 부르며, weights는 `loadings`라고 부른다.\n",
    "- PCA를 feature engineering에 적용하는 법\n",
    "    1. components에 MI scores를 계산하여 어떤 variation이 가장 target에 대해 예측력이 있는지 알아본다. 이를 바탕으로 features를 생성한다.\n",
    "    2. components 그 자체를 features로 사용한다.\n",
    "- PCA 적용 예시\n",
    "    - Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n",
    "    - Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n",
    "    - Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n",
    "    - Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(X, standardize=True):\n",
    "    # Standardize\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Create principal components\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    # Create loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,  # transpose the matrix of loadings\n",
    "        columns=component_names,  # so the columns are the principal components\n",
    "        index=X.columns,  # and the rows are the original features\n",
    "    )\n",
    "    return pca, X_pca, loadings"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
